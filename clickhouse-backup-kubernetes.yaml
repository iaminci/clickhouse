apiVersion: "clickhouse.altinity.com/v1"
kind: "ClickHouseInstallation"

metadata:
  name: "monitoring-demo"

spec:
  defaults:
    templates:
      podTemplate: clickhouse-backup
      dataVolumeClaimTemplate: clickhouse-storage-template

  configuration:
    users:
      # use cluster Pod CIDR for more security
      backup/networks/ip: 0.0.0.0/0
      # PASSWORD=backup_password; echo "$PASSWORD"; echo -n "$PASSWORD" | sha256sum | tr -d '-'
      backup/password_sha256_hex: eb94c11d77f46a0290ba8c4fca1a7fd315b72e1e6c83146e42117c568cc3ea4d
      backup/password: backup_password
    settings:
      # to allow scrape metrics via embedded prometheus protocol
      prometheus/endpoint: /metrics
      prometheus/port: 8888
      prometheus/metrics: true
      prometheus/events: true
      prometheus/asynchronous_metrics: true
    # need install zookeeper separately, look to https://github.com/Altinity/clickhouse-operator/tree/master/deploy/zookeeper/ for details
    zookeeper:
      nodes:
        - host: zookeeper.zoons
          port: 2181
      session_timeout_ms: 5000
      operation_timeout_ms: 5000
    users:
      # printf 'test_password' | sha256sum
      test_user/password_sha256_hex: 10a6e6cc8311a3e2bcc09bf6c199adecd5dd59408c343e926b129c4914f3cb01
      test_user/password: test_password
      # to allow access outside from kubernetes
      test_user/networks/ip:
        - 0.0.0.0/0
    clusters:
      - name: replcluster
        layout:
          # 2 shards one replica in each
          shardsCount: 1
          replicasCount: 1

  templates:

    volumeClaimTemplates:
      - name: clickhouse-storage-template
        spec:
          accessModes:
            - ReadWriteOnce
          resources:
            requests:
              storage: 50Gi
          storageClassName: local-path
          selector:
            matchLabels:
              type: clickhouse-data
    podTemplates:
      - name: clickhouse-backup
        metadata:
          annotations:
            prometheus.io/scrape: 'true'
            prometheus.io/port: '8888'
            prometheus.io/path: '/metrics'
            # need separate prometheus scrape config, look to https://github.com/prometheus/prometheus/issues/3756
            clickhouse.backup/scrape: 'true'
            clickhouse.backup/port: '7171'
            clickhouse.backup/path: '/metrics'
        spec:
          securityContext:
            runAsUser: 101
            runAsGroup: 101
            fsGroup: 101
          containers:
            - name: clickhouse-pod
              image: clickhouse/clickhouse-server:22.8
              command:
                - clickhouse-server
                - --config-file=/etc/clickhouse-server/config.xml
            - name: clickhouse-backup
              image: altinity/clickhouse-backup:latest
              imagePullPolicy: Always
              args: [ "server" ]
              env:
                - name: LOG_LEVEL
                  value: "debug"
                - name: ALLOW_EMPTY_BACKUPS
                  value: "true"
                - name: API_LISTEN
                  value: "0.0.0.0:7171"
                # INSERT INTO system.backup_actions to execute backup
                - name: API_CREATE_INTEGRATION_TABLES
                  value: "true"
                - name: BACKUPS_TO_KEEP_REMOTE
                  value: "5000"
                - name: BACKUPS_TO_KEEP_LOCAL
                  value: "3"
                # change it for production S3
                - name: REMOTE_STORAGE
                  value: "s3"
                - name: S3_ACL
                  value: "private"
                - name: S3_ENDPOINT
                  value: "http://minio.minio.svc.cluster.local:9000"
                - name: S3_REGION
                  value: ap-south-1
                - name: S3_BUCKET
                  value: clickhouse-backup
                # {shard} macro defined by clickhouse-operator
                # - name: S3_PATH
                #   value: backup/shard-{shard}
                - name: S3_ACCESS_KEY
                  value: "PY6t1xQQGcfoZ594tcHo"
                - name: S3_SECRET_KEY
                  value: "Qf65a8c8Tf4dCrE7quGADgYG1hnr8SwetZRTHDBU"
                - name: S3_FORCE_PATH_STYLE
                  value: "true"
                # remove it for production S3
                - name: S3_DISABLE_SSL
                  value: "true"
                - name: S3_DEBUG
                  value: "true"
                # require to avoid double scraping clickhouse and clickhouse-backup containers
              ports:
                - name: backup-rest
                  containerPort: 7171
---
apiVersion: v1
kind: PersistentVolume
metadata:
  name: pv-clickhouse-storage-template-0
  labels:
    type: clickhouse-data
spec:
  capacity:
    storage: 50Gi
  accessModes:
    - ReadWriteOnce
  persistentVolumeReclaimPolicy: Retain
  storageClassName: local-path
  hostPath:
    path: /mnt/clickhouse/datadir-0
    type: DirectoryOrCreate